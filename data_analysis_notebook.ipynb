{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1bfe7b",
   "metadata": {},
   "source": [
    "# UBS Client Conversation Analysis - SwissAIHacks25\n",
    "\n",
    "This notebook analyzes the UBS synthetic call transcripts dataset to understand the types of tasks extracted from client conversations and provides comprehensive visualizations of the data patterns.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The SwissAIHacks25 project tackles the challenge: **\"From Talk to Task: Insights from Client Conversations\"**. The goal is to use Generative AI to analyze transcripts of client conversations and automatically extract actionable insights such as client requests and action items.\n",
    "\n",
    "### Key Components:\n",
    "- **Dataset**: UBS synthetic call transcripts with ground truth task extractions\n",
    "- **Tasks**: Various banking operations like KYC updates, contact info changes, investment requests\n",
    "- **Languages**: Multilingual support (German, English, etc.)\n",
    "- **Models**: Comparison of different AI models for task extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e55660",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b50b134",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import Required Libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Python version: {np.__version__} (NumPy), {pd.__version__} (Pandas)\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7c996",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Repository Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path(\"data/ubs_synthetic_call_transcripts_dataset\")\n",
    "train_dir = data_dir / \"train\"\n",
    "test_dir = data_dir / \"test\"\n",
    "validation_dir = data_dir / \"validation\"\n",
    "\n",
    "# Function to load data from each split\n",
    "def load_dataset_split(split_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load all JSON and TXT files from a dataset split directory\"\"\"\n",
    "    json_files = list(split_dir.glob(\"*.json\"))\n",
    "    txt_files = list(split_dir.glob(\"*.txt\"))\n",
    "    \n",
    "    data = {\n",
    "        \"json_files\": [],\n",
    "        \"txt_files\": [],\n",
    "        \"conversations\": [],\n",
    "        \"tasks\": []\n",
    "    }\n",
    "    \n",
    "    # Load JSON files (ground truth tasks)\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                tasks_data = json.load(f)\n",
    "                data[\"tasks\"].append({\n",
    "                    \"file_id\": json_file.stem,\n",
    "                    \"tasks\": tasks_data\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    # Load TXT files (conversation transcripts)\n",
    "    for txt_file in txt_files:\n",
    "        try:\n",
    "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                data[\"conversations\"].append({\n",
    "                    \"file_id\": txt_file.stem,\n",
    "                    \"content\": content\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {txt_file}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load all dataset splits\n",
    "print(\"Loading dataset splits...\")\n",
    "train_data = load_dataset_split(train_dir)\n",
    "test_data = load_dataset_split(test_dir)\n",
    "validation_data = load_dataset_split(validation_dir)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Train: {len(train_data['tasks'])} task files, {len(train_data['conversations'])} conversation files\")\n",
    "print(f\"Test: {len(test_data['tasks'])} task files, {len(test_data['conversations'])} conversation files\") \n",
    "print(f\"Validation: {len(validation_data['tasks'])} task files, {len(validation_data['conversations'])} conversation files\")\n",
    "\n",
    "# Verify file matching\n",
    "print(f\"\\nFile matching verification:\")\n",
    "for split_name, split_data in [(\"Train\", train_data), (\"Test\", test_data), (\"Validation\", validation_data)]:\n",
    "    task_ids = {item['file_id'] for item in split_data['tasks']}\n",
    "    conv_ids = {item['file_id'] for item in split_data['conversations']}\n",
    "    matching = len(task_ids & conv_ids)\n",
    "    print(f\"{split_name}: {matching} files have both tasks and conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8442a0e",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and process conversation data\n",
    "def clean_conversation_text(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract and clean conversation data\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Remove disclaimer\n",
    "    clean_lines = []\n",
    "    skip_disclaimer = True\n",
    "    speakers = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip disclaimer section\n",
    "        if skip_disclaimer and \"Speaker\" not in line:\n",
    "            continue\n",
    "        skip_disclaimer = False\n",
    "        \n",
    "        # Extract speaker information\n",
    "        if line.startswith(\"Speaker\"):\n",
    "            speaker_match = re.match(r'Speaker (\\d+):', line)\n",
    "            if speaker_match:\n",
    "                speaker_id = speaker_match.group(1)\n",
    "                content = line[speaker_match.end():].strip()\n",
    "                speakers.append({\n",
    "                    'speaker_id': speaker_id,\n",
    "                    'content': content\n",
    "                })\n",
    "        \n",
    "        clean_lines.append(line)\n",
    "    \n",
    "    # Calculate conversation statistics\n",
    "    full_text = ' '.join(clean_lines)\n",
    "    word_count = len(full_text.split())\n",
    "    char_count = len(full_text)\n",
    "    speaker_count = len(set([s['speaker_id'] for s in speakers]))\n",
    "    \n",
    "    return {\n",
    "        'clean_text': full_text,\n",
    "        'speakers': speakers,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'speaker_count': speaker_count,\n",
    "        'line_count': len(clean_lines)\n",
    "    }\n",
    "\n",
    "# Process all conversations\n",
    "def process_all_conversations(data_splits):\n",
    "    \"\"\"Process conversations from all data splits\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for split_name, split_data in data_splits.items():\n",
    "        for conv in split_data['conversations']:\n",
    "            try:\n",
    "                processed_conv = clean_conversation_text(conv['content'])\n",
    "                processed_conv['file_id'] = conv['file_id']\n",
    "                processed_conv['split'] = split_name\n",
    "                processed_data.append(processed_conv)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing conversation {conv['file_id']}: {e}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Process conversations\n",
    "print(\"Processing conversations...\")\n",
    "data_splits = {\"train\": train_data, \"test\": test_data, \"validation\": validation_data}\n",
    "processed_conversations = process_all_conversations(data_splits)\n",
    "\n",
    "# Create DataFrame for conversations\n",
    "conv_df = pd.DataFrame(processed_conversations)\n",
    "print(f\"Processed {len(conv_df)} conversations\")\n",
    "print(f\"Columns: {conv_df.columns.tolist()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nConversation Statistics:\")\n",
    "print(conv_df[['word_count', 'char_count', 'speaker_count', 'line_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0a4d0",
   "metadata": {},
   "source": [
    "## 4. Task Type Classification and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and analyze task data\n",
    "def process_all_tasks(data_splits):\n",
    "    \"\"\"Process tasks from all data splits\"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    for split_name, split_data in data_splits.items():\n",
    "        for task_data in split_data['tasks']:\n",
    "            file_id = task_data['file_id']\n",
    "            tasks = task_data['tasks']\n",
    "            \n",
    "            # Handle empty task lists\n",
    "            if not tasks:\n",
    "                all_tasks.append({\n",
    "                    'file_id': file_id,\n",
    "                    'split': split_name,\n",
    "                    'task_type': 'no_tasks',\n",
    "                    'has_parameters': False,\n",
    "                    'parameter_count': 0,\n",
    "                    'parameters': {}\n",
    "                })\n",
    "            else:\n",
    "                for task in tasks:\n",
    "                    task_type = task.get('task_type', 'unknown')\n",
    "                    parameters = task.get('parameters', {})\n",
    "                    \n",
    "                    all_tasks.append({\n",
    "                        'file_id': file_id,\n",
    "                        'split': split_name,\n",
    "                        'task_type': task_type,\n",
    "                        'has_parameters': len(parameters) > 0,\n",
    "                        'parameter_count': len(parameters),\n",
    "                        'parameters': parameters\n",
    "                    })\n",
    "    \n",
    "    return all_tasks\n",
    "\n",
    "# Process all tasks\n",
    "print(\"Processing tasks...\")\n",
    "all_tasks = process_all_tasks(data_splits)\n",
    "tasks_df = pd.DataFrame(all_tasks)\n",
    "\n",
    "print(f\"Total tasks extracted: {len(tasks_df)}\")\n",
    "print(f\"Unique task types: {tasks_df['task_type'].nunique()}\")\n",
    "print(f\"Files with no tasks: {(tasks_df['task_type'] == 'no_tasks').sum()}\")\n",
    "\n",
    "# Analyze task types\n",
    "task_type_counts = tasks_df['task_type'].value_counts()\n",
    "print(f\"\\nTop 10 Task Types:\")\n",
    "print(task_type_counts.head(10))\n",
    "\n",
    "# Create task categories\n",
    "def categorize_task_type(task_type: str) -> str:\n",
    "    \"\"\"Categorize task types into broader categories\"\"\"\n",
    "    if 'kyc' in task_type.lower():\n",
    "        return 'KYC Updates'\n",
    "    elif 'contact' in task_type.lower():\n",
    "        return 'Contact Information'\n",
    "    elif 'investment' in task_type.lower() or 'portfolio' in task_type.lower():\n",
    "        return 'Investment Services'\n",
    "    elif 'account' in task_type.lower() or 'banking' in task_type.lower():\n",
    "        return 'Account Management'\n",
    "    elif 'document' in task_type.lower():\n",
    "        return 'Document Services'\n",
    "    elif 'advisory' in task_type.lower() or 'consultation' in task_type.lower():\n",
    "        return 'Advisory Services'\n",
    "    elif task_type == 'no_tasks':\n",
    "        return 'No Tasks'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "tasks_df['task_category'] = tasks_df['task_type'].apply(categorize_task_type)\n",
    "\n",
    "# Display task category distribution\n",
    "print(f\"\\nTask Category Distribution:\")\n",
    "category_counts = tasks_df['task_category'].value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e89ef0",
   "metadata": {},
   "source": [
    "## 5. Create Distribution Plots for Task Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive task distribution visualizations\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Task Category Distribution - Horizontal Bar Chart\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Task categories horizontal bar chart\n",
    "category_counts.plot(kind='barh', ax=ax1, color=sns.color_palette(\"viridis\", len(category_counts)))\n",
    "ax1.set_title('Distribution of Task Categories', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Number of Tasks')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(category_counts.values):\n",
    "    ax1.text(v + max(category_counts.values) * 0.01, i, str(v), \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# 2. Task Category Distribution - Pie Chart\n",
    "wedges, texts, autotexts = ax2.pie(category_counts.values, labels=category_counts.index, \n",
    "                                   autopct='%1.1f%%', startangle=90,\n",
    "                                   colors=sns.color_palette(\"husl\", len(category_counts)))\n",
    "ax2.set_title('Task Categories - Percentage Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Improve text readability\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 3. Top 15 Specific Task Types\n",
    "top_15_tasks = task_type_counts.head(15)\n",
    "bars = ax3.bar(range(len(top_15_tasks)), top_15_tasks.values, \n",
    "               color=sns.color_palette(\"coolwarm\", len(top_15_tasks)))\n",
    "ax3.set_title('Top 15 Specific Task Types', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Task Types')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_xticks(range(len(top_15_tasks)))\n",
    "ax3.set_xticklabels(top_15_tasks.index, rotation=45, ha='right')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + max(top_15_tasks.values) * 0.01,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Distribution across dataset splits\n",
    "split_category_crosstab = pd.crosstab(tasks_df['split'], tasks_df['task_category'])\n",
    "split_category_crosstab.plot(kind='bar', stacked=True, ax=ax4, \n",
    "                            color=sns.color_palette(\"Set3\", len(category_counts)))\n",
    "ax4.set_title('Task Categories by Dataset Split', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Dataset Split')\n",
    "ax4.set_ylabel('Number of Tasks')\n",
    "ax4.legend(title='Task Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax4.set_xticklabels(ax4.get_xticklabels(), rotation=0)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TASK DISTRIBUTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total tasks analyzed: {len(tasks_df):,}\")\n",
    "print(f\"Unique task types: {tasks_df['task_type'].nunique()}\")\n",
    "print(f\"Most common task category: {category_counts.index[0]} ({category_counts.iloc[0]} tasks)\")\n",
    "print(f\"Files with no tasks: {(tasks_df['task_type'] == 'no_tasks').sum()}\")\n",
    "print(f\"Files with parameters: {tasks_df['has_parameters'].sum()}\")\n",
    "print(f\"Average parameters per task: {tasks_df['parameter_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352c4f1",
   "metadata": {},
   "source": [
    "## 6. Generate Conversation Analysis Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversation characteristics and patterns\n",
    "\n",
    "# Create conversation analysis visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Word count distribution\n",
    "ax1.hist(conv_df['word_count'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(conv_df['word_count'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {conv_df[\"word_count\"].mean():.0f}')\n",
    "ax1.axvline(conv_df['word_count'].median(), color='green', linestyle='--', \n",
    "           label=f'Median: {conv_df[\"word_count\"].median():.0f}')\n",
    "ax1.set_title('Distribution of Conversation Word Counts', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Word Count')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Speaker count analysis\n",
    "speaker_counts = conv_df['speaker_count'].value_counts().sort_index()\n",
    "bars2 = ax2.bar(speaker_counts.index, speaker_counts.values, \n",
    "                color=sns.color_palette(\"coolwarm\", len(speaker_counts)))\n",
    "ax2.set_title('Distribution of Speaker Counts per Conversation', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Speakers')\n",
    "ax2.set_ylabel('Number of Conversations')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + max(speaker_counts.values) * 0.01,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Conversation length by dataset split\n",
    "conv_df.boxplot(column='word_count', by='split', ax=ax3)\n",
    "ax3.set_title('Conversation Length Distribution by Dataset Split', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Dataset Split')\n",
    "ax3.set_ylabel('Word Count')\n",
    "ax3.grid(alpha=0.3)\n",
    "plt.suptitle('')  # Remove the default title\n",
    "\n",
    "# 4. Character count vs word count relationship\n",
    "scatter = ax4.scatter(conv_df['word_count'], conv_df['char_count'], \n",
    "                     c=conv_df['speaker_count'], cmap='viridis', alpha=0.6)\n",
    "ax4.set_title('Character Count vs Word Count (colored by Speaker Count)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Word Count')\n",
    "ax4.set_ylabel('Character Count')\n",
    "plt.colorbar(scatter, ax=ax4, label='Number of Speakers')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(conv_df['word_count'], conv_df['char_count'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax4.plot(conv_df['word_count'], p(conv_df['word_count']), \"r--\", alpha=0.8, \n",
    "         label=f'Trend line (slope: {z[0]:.2f})')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print conversation statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONVERSATION ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total conversations: {len(conv_df):,}\")\n",
    "print(f\"Average word count: {conv_df['word_count'].mean():.1f}\")\n",
    "print(f\"Average character count: {conv_df['char_count'].mean():.1f}\")\n",
    "print(f\"Average speakers per conversation: {conv_df['speaker_count'].mean():.1f}\")\n",
    "print(f\"Most common speaker count: {conv_df['speaker_count'].mode().iloc[0]}\")\n",
    "print(f\"\\nWord count statistics:\")\n",
    "print(conv_df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865126ae",
   "metadata": {},
   "source": [
    "## 7. Visualize Task Complexity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge conversation and task data for complexity analysis\n",
    "# Create a mapping of file_id to conversation metrics\n",
    "conv_metrics = conv_df.set_index('file_id')[['word_count', 'char_count', 'speaker_count', 'split']].to_dict('index')\n",
    "\n",
    "# Add conversation metrics to tasks\n",
    "tasks_with_conv = []\n",
    "for _, task_row in tasks_df.iterrows():\n",
    "    file_id = task_row['file_id']\n",
    "    task_data = task_row.to_dict()\n",
    "    \n",
    "    if file_id in conv_metrics:\n",
    "        task_data.update(conv_metrics[file_id])\n",
    "    else:\n",
    "        task_data.update({'word_count': 0, 'char_count': 0, 'speaker_count': 0})\n",
    "    \n",
    "    tasks_with_conv.append(task_data)\n",
    "\n",
    "# Create enhanced dataframe\n",
    "enhanced_tasks_df = pd.DataFrame(tasks_with_conv)\n",
    "\n",
    "# Calculate task complexity metrics\n",
    "def calculate_task_complexity(row):\n",
    "    \"\"\"Calculate a complexity score based on various factors\"\"\"\n",
    "    complexity_score = 0\n",
    "    \n",
    "    # Parameter complexity (0-3 points)\n",
    "    if row['parameter_count'] == 0:\n",
    "        complexity_score += 0\n",
    "    elif row['parameter_count'] <= 2:\n",
    "        complexity_score += 1\n",
    "    elif row['parameter_count'] <= 4:\n",
    "        complexity_score += 2\n",
    "    else:\n",
    "        complexity_score += 3\n",
    "    \n",
    "    # Conversation length complexity (0-2 points)\n",
    "    word_count = row.get('word_count', 0)\n",
    "    if word_count > 200:\n",
    "        complexity_score += 2\n",
    "    elif word_count > 100:\n",
    "        complexity_score += 1\n",
    "    \n",
    "    # Multi-speaker complexity (0-1 point)\n",
    "    speaker_count = row.get('speaker_count', 0)\n",
    "    if speaker_count > 2:\n",
    "        complexity_score += 1\n",
    "    \n",
    "    return complexity_score\n",
    "\n",
    "enhanced_tasks_df['complexity_score'] = enhanced_tasks_df.apply(calculate_task_complexity, axis=1)\n",
    "\n",
    "# Create task complexity visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Box plot of parameter counts by task category\n",
    "enhanced_tasks_df.boxplot(column='parameter_count', by='task_category', ax=ax1)\n",
    "ax1.set_title('Parameter Count Distribution by Task Category', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Task Category')\n",
    "ax1.set_ylabel('Number of Parameters')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "# 2. Complexity score distribution\n",
    "complexity_counts = enhanced_tasks_df['complexity_score'].value_counts().sort_index()\n",
    "bars3 = ax2.bar(complexity_counts.index, complexity_counts.values, \n",
    "                color=sns.color_palette(\"rocket\", len(complexity_counts)))\n",
    "ax2.set_title('Task Complexity Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Complexity Score (0-6)')\n",
    "ax2.set_ylabel('Number of Tasks')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + max(complexity_counts.values) * 0.01,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Scatter plot: Conversation length vs Task complexity\n",
    "# Filter out tasks with no conversation data\n",
    "filtered_df = enhanced_tasks_df[enhanced_tasks_df['word_count'] > 0]\n",
    "scatter2 = ax3.scatter(filtered_df['word_count'], filtered_df['complexity_score'], \n",
    "                      c=filtered_df['parameter_count'], cmap='plasma', alpha=0.6, s=30)\n",
    "ax3.set_title('Conversation Length vs Task Complexity', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Conversation Word Count')\n",
    "ax3.set_ylabel('Task Complexity Score')\n",
    "plt.colorbar(scatter2, ax=ax3, label='Parameter Count')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Violin plot of complexity by task category\n",
    "# Filter categories with enough data\n",
    "categories_with_data = enhanced_tasks_df['task_category'].value_counts()\n",
    "top_categories = categories_with_data[categories_with_data >= 10].index\n",
    "filtered_for_violin = enhanced_tasks_df[enhanced_tasks_df['task_category'].isin(top_categories)]\n",
    "\n",
    "if len(filtered_for_violin) > 0:\n",
    "    sns.violinplot(data=filtered_for_violin, x='task_category', y='complexity_score', ax=ax4)\n",
    "    ax4.set_title('Task Complexity Distribution by Category (Violin Plot)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Task Category')\n",
    "    ax4.set_ylabel('Complexity Score')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data for violin plot', \n",
    "             transform=ax4.transAxes, ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Task Complexity Distribution by Category', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print complexity analysis\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TASK COMPLEXITY ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Average complexity score: {enhanced_tasks_df['complexity_score'].mean():.2f}\")\n",
    "print(f\"Most complex task category: {enhanced_tasks_df.groupby('task_category')['complexity_score'].mean().idxmax()}\")\n",
    "print(f\"Simplest task category: {enhanced_tasks_df.groupby('task_category')['complexity_score'].mean().idxmin()}\")\n",
    "print(f\"\\nComplexity score breakdown:\")\n",
    "for score in sorted(enhanced_tasks_df['complexity_score'].unique()):\n",
    "    count = (enhanced_tasks_df['complexity_score'] == score).sum()\n",
    "    percentage = count / len(enhanced_tasks_df) * 100\n",
    "    print(f\"  Score {score}: {count} tasks ({percentage:.1f}%)\")\n",
    "    \n",
    "# Show average complexity by category\n",
    "print(f\"\\nAverage complexity by task category:\")\n",
    "complexity_by_category = enhanced_tasks_df.groupby('task_category')['complexity_score'].mean().sort_values(ascending=False)\n",
    "for category, avg_complexity in complexity_by_category.items():\n",
    "    count = (enhanced_tasks_df['task_category'] == category).sum()\n",
    "    print(f\"  {category}: {avg_complexity:.2f} (n={count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf070e",
   "metadata": {},
   "source": [
    "## 8. Create Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fba150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation analysis\n",
    "# Select numeric columns for correlation analysis\n",
    "numeric_columns = ['word_count', 'char_count', 'speaker_count', 'parameter_count', 'complexity_score']\n",
    "correlation_data = enhanced_tasks_df[numeric_columns].fillna(0)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Create correlation heatmaps\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Main correlation heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax1, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "ax1.set_title('Correlation Matrix - Task and Conversation Metrics', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Task category vs metrics heatmap\n",
    "# Create a pivot table for task categories and metrics\n",
    "category_metrics = enhanced_tasks_df.groupby('task_category')[numeric_columns].mean()\n",
    "\n",
    "# Normalize the data for better visualization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "category_metrics_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(category_metrics),\n",
    "    index=category_metrics.index,\n",
    "    columns=category_metrics.columns\n",
    ")\n",
    "\n",
    "sns.heatmap(category_metrics_normalized.T, annot=True, cmap='viridis', \n",
    "            cbar_kws={'label': 'Standardized Score'}, ax=ax2)\n",
    "ax2.set_title('Task Categories vs Metrics (Standardized)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Task Category')\n",
    "ax2.set_ylabel('Metrics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional correlation analysis\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Strong correlations (|r| > 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.5:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            print(f\"  {var1} ‚Üî {var2}: {corr_value:.3f}\")\n",
    "\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"  ‚Ä¢ Word count and character count correlation: {correlation_matrix.loc['word_count', 'char_count']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Complexity score and parameter count correlation: {correlation_matrix.loc['complexity_score', 'parameter_count']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Speaker count and word count correlation: {correlation_matrix.loc['speaker_count', 'word_count']:.3f}\")\n",
    "\n",
    "# Show category-specific statistics\n",
    "print(f\"\\nTask Category Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "for category in category_metrics.index:\n",
    "    n_tasks = (enhanced_tasks_df['task_category'] == category).sum()\n",
    "    avg_complexity = category_metrics.loc[category, 'complexity_score']\n",
    "    avg_params = category_metrics.loc[category, 'parameter_count']\n",
    "    avg_words = category_metrics.loc[category, 'word_count']\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  Tasks: {n_tasks}\")\n",
    "    print(f\"  Avg Complexity: {avg_complexity:.2f}\")\n",
    "    print(f\"  Avg Parameters: {avg_params:.2f}\")\n",
    "    print(f\"  Avg Word Count: {avg_words:.0f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19a6ea",
   "metadata": {},
   "source": [
    "## 9. Generate Summary Statistics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary dashboard\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Create a grid layout for the dashboard\n",
    "gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Dataset overview (top left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "dataset_sizes = [len(train_data['tasks']), len(test_data['tasks']), len(validation_data['tasks'])]\n",
    "colors1 = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "wedges, texts, autotexts = ax1.pie(dataset_sizes, labels=['Train', 'Test', 'Validation'], \n",
    "                                   autopct='%1.1f%%', colors=colors1, startangle=90)\n",
    "ax1.set_title('Dataset Split Distribution', fontweight='bold')\n",
    "\n",
    "# 2. Task category summary (top middle-left)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "top_5_categories = category_counts.head(5)\n",
    "bars2 = ax2.barh(range(len(top_5_categories)), top_5_categories.values, color=sns.color_palette(\"viridis\", 5))\n",
    "ax2.set_yticks(range(len(top_5_categories)))\n",
    "ax2.set_yticklabels(top_5_categories.index)\n",
    "ax2.set_title('Top 5 Task Categories', fontweight='bold')\n",
    "ax2.set_xlabel('Count')\n",
    "for i, v in enumerate(top_5_categories.values):\n",
    "    ax2.text(v + max(top_5_categories.values) * 0.02, i, str(v), va='center')\n",
    "\n",
    "# 3. Conversation metrics (top middle-right)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "metrics_data = [conv_df['word_count'].mean(), conv_df['char_count'].mean()/10, \n",
    "                conv_df['speaker_count'].mean()*50]\n",
    "metrics_labels = ['Avg Words', 'Avg Chars/10', 'Avg Speakers√ó50']\n",
    "bars3 = ax3.bar(metrics_labels, metrics_data, color=['#FF9F40', '#36A2EB', '#4BC0C0'])\n",
    "ax3.set_title('Conversation Metrics', fontweight='bold')\n",
    "ax3.set_ylabel('Value')\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Key statistics (top right)\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "ax4.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "KEY STATISTICS\n",
    "\n",
    "Total Files: {len(conv_df):,}\n",
    "Total Tasks: {len(tasks_df):,}\n",
    "Unique Task Types: {tasks_df['task_type'].nunique()}\n",
    "\n",
    "Tasks with Parameters: {tasks_df['has_parameters'].sum():,}\n",
    "Empty Task Files: {(tasks_df['task_type'] == 'no_tasks').sum():,}\n",
    "\n",
    "Avg Words/Conversation: {conv_df['word_count'].mean():.0f}\n",
    "Max Word Count: {conv_df['word_count'].max():,}\n",
    "Min Word Count: {conv_df['word_count'].min():,}\n",
    "\"\"\"\n",
    "ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "\n",
    "# 5. Word count distribution (second row, span 2 columns)\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "ax5.hist(conv_df['word_count'], bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax5.axvline(conv_df['word_count'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax5.axvline(conv_df['word_count'].median(), color='blue', linestyle='--', linewidth=2)\n",
    "ax5.set_title('Word Count Distribution Across All Conversations', fontweight='bold')\n",
    "ax5.set_xlabel('Word Count')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.legend(['Mean', 'Median', 'Distribution'])\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Complexity distribution (second row, right side)\n",
    "ax6 = fig.add_subplot(gs[1, 2:])\n",
    "complexity_by_category = enhanced_tasks_df.groupby('task_category')['complexity_score'].mean().sort_values()\n",
    "bars6 = ax6.bar(range(len(complexity_by_category)), complexity_by_category.values, \n",
    "                color=sns.color_palette(\"rocket\", len(complexity_by_category)))\n",
    "ax6.set_title('Average Task Complexity by Category', fontweight='bold')\n",
    "ax6.set_xticks(range(len(complexity_by_category)))\n",
    "ax6.set_xticklabels(complexity_by_category.index, rotation=45, ha='right')\n",
    "ax6.set_ylabel('Average Complexity Score')\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 7. Task distribution across splits (third row, left)\n",
    "ax7 = fig.add_subplot(gs[2, :2])\n",
    "split_counts = enhanced_tasks_df['split'].value_counts()\n",
    "bars7 = ax7.bar(split_counts.index, split_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax7.set_title('Task Distribution Across Dataset Splits', fontweight='bold')\n",
    "ax7.set_ylabel('Number of Tasks')\n",
    "for bar in bars7:\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 8. Parameter analysis (third row, right)\n",
    "ax8 = fig.add_subplot(gs[2, 2:])\n",
    "param_counts = enhanced_tasks_df['parameter_count'].value_counts().sort_index()\n",
    "bars8 = ax8.bar(param_counts.index, param_counts.values, color='skyblue', edgecolor='navy')\n",
    "ax8.set_title('Distribution of Parameter Counts per Task', fontweight='bold')\n",
    "ax8.set_xlabel('Number of Parameters')\n",
    "ax8.set_ylabel('Number of Tasks')\n",
    "ax8.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 9. Language analysis (bottom row - analyze text patterns)\n",
    "ax9 = fig.add_subplot(gs[3, :2])\n",
    "# Simple language detection based on common words\n",
    "def detect_language_simple(text):\n",
    "    \"\"\"Simple language detection based on common German words\"\"\"\n",
    "    german_words = ['der', 'die', 'das', 'und', 'ich', 'sie', 'ist', 'mit', 'zu', 'von', 'auf']\n",
    "    english_words = ['the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that', 'by']\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    german_score = sum(1 for word in german_words if word in text_lower)\n",
    "    english_score = sum(1 for word in english_words if word in text_lower)\n",
    "    \n",
    "    if german_score > english_score:\n",
    "        return 'German'\n",
    "    elif english_score > german_score:\n",
    "        return 'English'\n",
    "    else:\n",
    "        return 'Mixed/Other'\n",
    "\n",
    "# Apply language detection to a sample of conversations\n",
    "sample_convs = conv_df.sample(min(200, len(conv_df)))\n",
    "sample_convs['detected_language'] = sample_convs['clean_text'].apply(detect_language_simple)\n",
    "lang_counts = sample_convs['detected_language'].value_counts()\n",
    "\n",
    "bars9 = ax9.bar(lang_counts.index, lang_counts.values, color=['#FFA07A', '#98D8C8', '#F7DC6F'])\n",
    "ax9.set_title(f'Detected Languages (Sample of {len(sample_convs)} conversations)', fontweight='bold')\n",
    "ax9.set_ylabel('Number of Conversations')\n",
    "for bar in bars9:\n",
    "    height = bar.get_height()\n",
    "    ax9.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 10. Model performance insights (bottom right)\n",
    "ax10 = fig.add_subplot(gs[3, 2:])\n",
    "ax10.axis('off')\n",
    "insights_text = f\"\"\"\n",
    "MODEL INSIGHTS & RECOMMENDATIONS\n",
    "\n",
    "üéØ TASK EXTRACTION CHALLENGES:\n",
    "‚Ä¢ {(tasks_df['task_type'] == 'no_tasks').sum()}/{len(tasks_df)} conversations have no extractable tasks\n",
    "‚Ä¢ Avg {enhanced_tasks_df['parameter_count'].mean():.1f} parameters per task\n",
    "‚Ä¢ Most complex category: {complexity_by_category.index[-1]}\n",
    "\n",
    "üìä DATA CHARACTERISTICS:\n",
    "‚Ä¢ Multilingual dataset (German dominant)\n",
    "‚Ä¢ Variable conversation lengths (50-500+ words)\n",
    "‚Ä¢ {conv_df['speaker_count'].mode().iloc[0]}-speaker conversations most common\n",
    "\n",
    "üîß MODEL RECOMMENDATIONS:\n",
    "‚Ä¢ Focus on {category_counts.index[0]} tasks (most frequent)\n",
    "‚Ä¢ Handle empty task cases ({(tasks_df['task_type'] == 'no_tasks').sum()} cases)\n",
    "‚Ä¢ Consider conversation length in complexity scoring\n",
    "‚Ä¢ Multilingual model required for production\n",
    "\n",
    "üìà EVALUATION METRICS:\n",
    "‚Ä¢ Test on {len(test_data['tasks'])} files\n",
    "‚Ä¢ Validate on {len(validation_data['tasks'])} files\n",
    "‚Ä¢ Consider task-specific F1 scores\n",
    "\"\"\"\n",
    "ax10.text(0.02, 0.98, insights_text, transform=ax10.transAxes, fontsize=9,\n",
    "          verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "plt.suptitle('UBS Client Conversation Analysis - Complete Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéØ FINAL ANALYSIS SUMMARY - SWISSAIHACKS25\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìä DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Total conversations analyzed: {len(conv_df):,}\")\n",
    "print(f\"   ‚Ä¢ Total tasks extracted: {len(tasks_df):,}\")\n",
    "print(f\"   ‚Ä¢ Unique task types identified: {tasks_df['task_type'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Dataset splits: {len(train_data['tasks'])} train, {len(test_data['tasks'])} test, {len(validation_data['tasks'])} validation\")\n",
    "print(f\"\\nüèÜ KEY FINDINGS:\")\n",
    "print(f\"   ‚Ä¢ Most common task category: {category_counts.index[0]} ({category_counts.iloc[0]} instances)\")\n",
    "print(f\"   ‚Ä¢ Average conversation length: {conv_df['word_count'].mean():.0f} words\")\n",
    "print(f\"   ‚Ä¢ {(tasks_df['task_type'] == 'no_tasks').sum()} conversations have no extractable tasks\")\n",
    "print(f\"   ‚Ä¢ Tasks with parameters: {tasks_df['has_parameters'].sum()}/{len(tasks_df)} ({tasks_df['has_parameters'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nüéØ MODEL DEVELOPMENT INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Focus areas: {', '.join(category_counts.head(3).index)}\")\n",
    "print(f\"   ‚Ä¢ Complexity range: 0-6 (avg: {enhanced_tasks_df['complexity_score'].mean():.2f})\")\n",
    "print(f\"   ‚Ä¢ Multilingual support needed (German/English detected)\")\n",
    "print(f\"   ‚Ä¢ Consider conversation length in task extraction performance\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
